{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes Ecommerce data from Kaggle and uses it to try to identify cancellations using Logistic Regression\n",
    "    1. Cleanses data\n",
    "            Removes all NAs\n",
    "            amends dates to String with correct format\n",
    "            \n",
    "    2. Calculates additional features: \n",
    "            Month of Year\n",
    "            Day of Week\n",
    "    3. Calculates label \n",
    "            Invoice Code like 'C%'\n",
    "    3a. Filters data set down to only the CustomerID with highest number of Cancellations\n",
    "    (this step was added after the initial model predicted no cancellatins whatsoever)\n",
    "    4. Indexes String features (inc error handling)\n",
    "    5. 'Hot Encodes' categorical features inc. those from 4.\n",
    "    4. Creates 'pipeline' \n",
    "            indexing\n",
    "            encoding\n",
    "            vector assembly\n",
    "            logistic regression model\n",
    "    5. Fits model to training data\n",
    "    8. Applies model to test data\n",
    "    9. Basic evaluation: model predicts no cancellations whatsoever even after filtering to customer\n",
    "    with the most cancellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- CustID: integer (nullable = true)\n",
      " |-- Val: double (nullable = true)\n",
      " |-- Label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country',\n",
       " 'Date',\n",
       " 'DayOfWeek',\n",
       " 'Month',\n",
       " 'CustID',\n",
       " 'Val',\n",
       " 'Label']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, IntegerType, StructType, StringType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "#import (format_number,dayofmonth,hour,dayofyear,month,\n",
    " #                                  year,weekofyear,date_format,concat, lit, from_unixtime\n",
    "  #                                 , unix_timestamp, to_date, sum as sm, format_number as fn\n",
    "   #                               ,bround, avg as av, udf, desc)\n",
    "#from pyspark.sql.functions import DayofYear as dow\n",
    "import numpy as np\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "df = spark.read.csv('EcommerceData.csv',inferSchema=True,header=True)\n",
    "\n",
    "df = df.na.drop()\n",
    "df = df.withColumn('Date', F.to_date(F.from_unixtime(F.unix_timestamp(\n",
    "    F.concat(df['InvoiceDate'],F.lit('')),format='MM/d/yyyy HH:mm'))))\n",
    "#df = df.withColumn('DayOfWeek', F.date_format(F.from_unixtime(F.unix_timestamp(df.InvoiceDate)),'EEEE'))\n",
    "df = df.withColumn('DayOfWeek', F.date_format(df.Date,'u').cast('integer'))\n",
    "df = df.withColumn('Month', F.month(df['Date']))\n",
    "#df = df.withColumn('Unix', F.unix_timestamp(df['Date'],format='yyyy/MM/dd'))\n",
    "gred = df.groupBy('CustomerID', 'Date').count()\n",
    "gg = gred.groupBy('CustomerID').count().filter('count>39').drop('count').withColumnRenamed('CustomerID','CustID')\n",
    "df = df.join(gg,gg.CustID==df.CustomerID)\n",
    "df = df.withColumn('Val', F.bround(df['Quantity']* df['Unitprice'],2))\n",
    "\n",
    "uErr = F.udf(lambda col: 1 if 'C' in col else 0, IntegerType())\n",
    "df = df.withColumn('Label',uErr('InvoiceNo'))\n",
    "df.printSchema()\n",
    "df.columns\n",
    "#df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|CustomerID|Label|count|\n",
      "+----------+-----+-----+\n",
      "|     14911|    1|  226|\n",
      "|     17841|    1|  136|\n",
      "|     15311|    1|  112|\n",
      "|     13798|    1|   90|\n",
      "|     14606|    1|   82|\n",
      "|     12748|    1|   46|\n",
      "|     13089|    1|   39|\n",
      "|     14527|    1|   39|\n",
      "|     16029|    1|   32|\n",
      "|     13767|    1|   31|\n",
      "|     16422|    1|   30|\n",
      "|     16133|    1|   26|\n",
      "|     13408|    1|   23|\n",
      "|     13078|    1|   22|\n",
      "|     17811|    1|   21|\n",
      "|     14156|    1|   20|\n",
      "|     15189|    1|   20|\n",
      "|     13694|    1|   17|\n",
      "|     15039|    1|    6|\n",
      "|     14646|    1|    5|\n",
      "+----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('CustomerID', 'Label').count().orderBy(F.desc('Label'),F.desc('count')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['CustomerID']=='14911')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Month: int, StockCode: string, DayOfWeek: int, Label: int, StockIndex: double, StockVec: vector, MonVec: vector, WDVec: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)\n",
    "#features: StockCode, Day of Week, Month\n",
    "my_final_data = df.select('Month','StockCode','DayOfWeek','Label')\n",
    "Stock_indexer = StringIndexer(inputCol='StockCode',outputCol='StockIndex').setHandleInvalid(\"skip\")\n",
    "Stock_encoder = OneHotEncoder(inputCol='StockIndex',outputCol='StockVec')\n",
    "#Mon_indexer = StringIndexer(inputCol='Month',outputCol='MonIndex').setHandleInvalid(\"skip\")\n",
    "Mon_encoder = OneHotEncoder(inputCol='Month',outputCol='MonVec')\n",
    "WD_encoder = OneHotEncoder(inputCol='DayOfWeek',outputCol='WDVec')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['StockVec',\n",
    " 'MonVec',\n",
    " 'WDVec'],outputCol='features')\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression as lr\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr_err = lr(maxIter=10,regParam=0.3,featuresCol='features',labelCol='Label')\n",
    "\n",
    "pipeline = Pipeline(stages=[Stock_indexer,\n",
    "                           Stock_encoder,Mon_encoder,WD_encoder,\n",
    "                           assembler,lr_err])\n",
    "train_data, test_data = my_final_data.randomSplit([0.8,.2])\n",
    "fit_model = pipeline.fit(train_data)\n",
    "results = fit_model.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Label')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Label: integer (nullable = true)\n",
      " |-- StockIndex: double (nullable = true)\n",
      " |-- CustIndex: double (nullable = true)\n",
      " |-- StockVec: vector (nullable = true)\n",
      " |-- CustVec: vector (nullable = true)\n",
      " |-- WDVec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 1017|\n",
      "+----------+-----+\n",
      "\n",
      "+-----+---------+---------+-----+----------+-------------------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|Month|StockCode|DayOfWeek|Label|StockIndex|           StockVec|        MonVec|        WDVec|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+---------+---------+-----+----------+-------------------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|    1|   18097C|        2|    0|     827.0| (1634,[827],[1.0])|(12,[1],[1.0])|(7,[2],[1.0])|(1653,[827,1635,1...|[3.24808755191550...|[0.96260433068897...|       0.0|\n",
      "|    1|    20914|        3|    1|      30.0|  (1634,[30],[1.0])|(12,[1],[1.0])|(7,[3],[1.0])|(1653,[30,1635,16...|[2.44479750392560...|[0.92018016896627...|       0.0|\n",
      "|    1|    21035|        7|    0|     756.0| (1634,[756],[1.0])|(12,[1],[1.0])|    (7,[],[])|(1653,[756,1635],...|[3.24202800118989...|[0.96238559113242...|       0.0|\n",
      "|    1|    21974|        4|    0|     707.0| (1634,[707],[1.0])|(12,[1],[1.0])|(7,[4],[1.0])|(1653,[707,1635,1...|[3.17861682828471...|[0.96002161352275...|       0.0|\n",
      "|    1|    22102|        7|    0|     811.0| (1634,[811],[1.0])|(12,[1],[1.0])|    (7,[],[])|(1653,[811,1635],...|[3.23897225804864...|[0.96227481813114...|       0.0|\n",
      "|    1|    22424|        1|    0|     229.0| (1634,[229],[1.0])|(12,[1],[1.0])|(7,[1],[1.0])|(1653,[229,1635,1...|[3.30488903219982...|[0.96459615297630...|       0.0|\n",
      "|    1|    22425|        4|    0|     420.0| (1634,[420],[1.0])|(12,[1],[1.0])|(7,[4],[1.0])|(1653,[420,1635,1...|[3.17689622347888...|[0.95995552421787...|       0.0|\n",
      "|    1|    22501|        4|    0|     305.0| (1634,[305],[1.0])|(12,[1],[1.0])|(7,[4],[1.0])|(1653,[305,1635,1...|[3.18129794240907...|[0.96012438817064...|       0.0|\n",
      "|    1|    22720|        1|    0|      20.0|  (1634,[20],[1.0])|(12,[1],[1.0])|(7,[1],[1.0])|(1653,[20,1635,16...|[3.10330153923038...|[0.95702872507257...|       0.0|\n",
      "|    1|    22720|        4|    0|      20.0|  (1634,[20],[1.0])|(12,[1],[1.0])|(7,[4],[1.0])|(1653,[20,1635,16...|[2.98284702372112...|[0.95179316922915...|       0.0|\n",
      "|    1|    22720|        4|    0|      20.0|  (1634,[20],[1.0])|(12,[1],[1.0])|(7,[4],[1.0])|(1653,[20,1635,16...|[2.98284702372112...|[0.95179316922915...|       0.0|\n",
      "|    1|    22722|        1|    0|    1631.0|(1634,[1631],[1.0])|(12,[1],[1.0])|(7,[1],[1.0])|(1653,[1631,1635,...|[3.32751989879641...|[0.96536093275603...|       0.0|\n",
      "|    1|    22740|        3|    0|    1037.0|(1634,[1037],[1.0])|(12,[1],[1.0])|(7,[3],[1.0])|(1653,[1037,1635,...|[3.15136383223652...|[0.95896242696698...|       0.0|\n",
      "|    1|    22753|        3|    0|     600.0| (1634,[600],[1.0])|(12,[1],[1.0])|(7,[3],[1.0])|(1653,[600,1635,1...|[3.17659666026107...|[0.95994400714666...|       0.0|\n",
      "|    1|    22847|        1|    0|     125.0| (1634,[125],[1.0])|(12,[1],[1.0])|(7,[1],[1.0])|(1653,[125,1635,1...|[3.30407797522461...|[0.96456844460492...|       0.0|\n",
      "|    1|    22983|        3|    0|    1492.0|(1634,[1492],[1.0])|(12,[1],[1.0])|(7,[3],[1.0])|(1653,[1492,1635,...|[3.14894059246291...|[0.95886695789155...|       0.0|\n",
      "|    1|    22984|        3|    0|     825.0| (1634,[825],[1.0])|(12,[1],[1.0])|(7,[3],[1.0])|(1653,[825,1635,1...|[3.17764712502561...|[0.95998437959350...|       0.0|\n",
      "|    1|    22985|        3|    0|     721.0| (1634,[721],[1.0])|(12,[1],[1.0])|(7,[3],[1.0])|(1653,[721,1635,1...|[3.14970485582737...|[0.95889709072186...|       0.0|\n",
      "|    1|    71459|        2|    0|     275.0| (1634,[275],[1.0])|(12,[1],[1.0])|(7,[2],[1.0])|(1653,[275,1635,1...|[3.25783626258952...|[0.96295367904428...|       0.0|\n",
      "|    1|   85035C|        4|    0|    1276.0|(1634,[1276],[1.0])|(12,[1],[1.0])|(7,[4],[1.0])|(1653,[1276,1635,...|[3.18672237709612...|[0.96033154806064...|       0.0|\n",
      "+-----+---------+---------+-----+----------+-------------------+--------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#results = results.withColumn('rprediction',F.bround('prediction',1))\n",
    "#results.show()\n",
    "results.groupby('prediction').count().show()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o251.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 251, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 84614A.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 84614A.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-975c92d0828d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#my_eval.evaluate(results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o251.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 251, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 84614A.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 84614A.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import FloatType, BooleanType\n",
    "def absn(n):\n",
    "    return abs(n)\n",
    "uAbs = F.udf(absn,FloatType())\n",
    "df = df.withColumn('AbsVal',uAbs('Val') )\n",
    "df = df.orderBy('CustomerID','StockCode','AbsVal',desc('Val'))\n",
    "win1 = Window.partitionBy('CustomerID','StockCode').orderBy('AbsVal',desc('Val')).rowsBetween(-1,0)\n",
    "df = df.withColumn('RevTot',F.sum('Val').over(win1).cast('Float'))\n",
    "def revcheck(a,b):\n",
    "    if (a<0) & (b==0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "uRevCheck = F.udf(revcheck)\n",
    "#df.show(1)\n",
    "df = df.withColumn('RevFlag',uRevCheck('Val','RevTot'))\n",
    "#creates a LIKE filter function\n",
    "likef = F.udf(lambda col: True if 'C' in col else False, BooleanType())\n",
    "df.filter(likef('InvoiceNo')).select('CustomerID','InvoiceNo','StockCode','Description','Val','AbsVal','RevTot').show(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#results =results.withColumn('Label',results.prediction.cast('double'))\n",
    "results = results.withColumn('rprediction',F.bround('prediction',1))\n",
    "#results.show()\n",
    "results.groupby(results.rprediction).count().show()\n",
    "#results.filter('prediction<>0').select('Label','prediction').show(5)\n",
    "#a.groupBy('prediction').count().show()\n",
    "#results.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Year and Day of Year column in order to see transactin volume over 10days\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "#import sys\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Import VectorAssembler for use later on\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "#remove refunds\n",
    "def isErr(s):\n",
    "    if s=='M':\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "uIsErr = udf(isErr)\n",
    "#fdf = df.filter(\"Val is not null and Val >= 0 and StockCode <> 'M'\")\n",
    "fdf = df.withColumn('Manual', uIsErr('StockCode'))\n",
    "\n",
    "#fdf = fdf.groupBy(['CustomerID','Date', 'Unix','Label']).agg({'Val':'sum'})\n",
    "#fdf = fdf.withColumnRenamed('sum(Val)','Val')\n",
    "#fdf = fdf.withColumn('Val', bround(fdf['Val'],scale=2))\n",
    "\n",
    "fdf = fdf.withColumn('DayOfYear', dayofyear(fdf['Date']))\n",
    "fdf = fdf.withColumn('Month', month(fdf['Date']))\n",
    "\n",
    "#define offset window to capture activity in past 10 days\n",
    "days = lambda x: x *86400\n",
    "w = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(28),-days(1))\n",
    "w2 = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(126),-days(1))\n",
    "\n",
    "#Calc average daily spend over short 1 and long 1 periods\n",
    "fdf = fdf.withColumn('S1Tot',bround(sm(fdf['Val']).over(w),scale=2))\n",
    "fdf = fdf.withColumn('L1Tot',bround(sm(fdf['Val']).over(w2),scale=2))\n",
    "fdf = fdf.na.fill(0,subset='S1Tot')\n",
    "fdf = fdf.na.fill(0,subset='L1Tot')\n",
    "fdf = fdf.withColumn('L1-S1Tot', bround(fdf['L1Tot']-fdf['S1Tot'],2))\n",
    "\n",
    "#Calc average daily spend over short 1 and long 1 periods\n",
    "fdf = fdf.withColumn('S1Avg',bround(av(fdf['Val']).over(w),scale=2))\n",
    "fdf = fdf.withColumn('L1Avg',bround(av(fdf['Val']).over(w2),scale=2))\n",
    "fdf = fdf.na.fill(0,subset='S1Avg')\n",
    "fdf = fdf.na.fill(0,subset='L1Avg')\n",
    "fdf = fdf.withColumn('L1-S1Avg', bround(fdf['L1Avg']-fdf['S1Avg'],2))\n",
    "\n",
    "w3 = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(7),-days(1))\n",
    "w4 = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(28),-days(1))\n",
    "\n",
    "#Calc average daily spend over short 1 and long 1 periods\n",
    "fdf = fdf.withColumn('S2Avg',bround(av(fdf['Val']).over(w3),scale=2))\n",
    "fdf = fdf.withColumn('L2Avg',bround(av(fdf['Val']).over(w4),scale=2))\n",
    "fdf = fdf.na.fill(0,subset='S2Avg')\n",
    "fdf = fdf.na.fill(0,subset='L2Avg')\n",
    "fdf = fdf.withColumn('L2-S2Avg', bround(fdf['L2Avg']-fdf['S2Avg'],2))\n",
    "\n",
    "#Create df containing only customers with >40 transactions\n",
    "gred = fdf.groupBy('CustomerID', 'Date').count()\n",
    "gg = gred.groupBy('CustomerID').count().filter('count>39').drop('count')\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    \n",
    "    inputCols=[\"L1-S1Tot\", \"Month\",\"L1-S1Avg\",\"L2-S2Avg\"],\n",
    "    outputCol=\"features\")\n",
    "#gg.count()B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19202"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.filter(fdf.Label==0).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-------------+-----+-------------+-------------+\n",
      "|CustomerID|  r2|RMSE|pVal_L1-S1Tot|Month|pVal_L1-S1Avg|pVal_L2-S2Avg|\n",
      "+----------+----+----+-------------+-----+-------------+-------------+\n",
      "|         1|2.01|2.01|         2.01| 2.01|         2.01|         2.01|\n",
      "+----------+----+----+-------------+-----+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#create list to add columns for results to the 'gg' DF\n",
    "metricCols =['r2', 'RMSE', 'pVal_L1-S1Tot','Month','pVal_L1-S1Avg', 'pVal_L2-S2Avg']\n",
    "\n",
    "columnlist = ['CustomerID'] + metricCols\n",
    "datal = [1,2.01,2.01,2.01,2.01,2.01,2.01]\n",
    "\n",
    "schema1 = [StructField(columnlist[0],IntegerType(),True)\n",
    "           , StructField(columnlist[1], FloatType(),True)\n",
    "          , StructField(columnlist[2], FloatType(),True)\n",
    "          , StructField(columnlist[3], FloatType(),True)\n",
    "          , StructField(columnlist[4], FloatType(),True)\n",
    "          , StructField(columnlist[5], FloatType(),True)\n",
    "          , StructField(columnlist[6], FloatType(),True)\n",
    "            ]\n",
    "\n",
    "dataa = sc.parallelize([datal])\n",
    "schema2 = StructType(fields=schema1)\n",
    "resultdf = spark.createDataFrame(dataa,schema2)\n",
    "resultdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of customers from df containing cust with >40 days of transactions\n",
    "custList = [x[0] for x in gg.select('CustomerID').collect()]\n",
    "#iterate through list running Linear Regression Model for each one\n",
    "for c in custList:\n",
    "    final = fdf.filter(fdf['CustomerID']==c).withColumnRenamed('Val','Label')\n",
    "    final = final.select('L1-S1Tot','Month','L1-S1Avg','L2-S2Avg','Label')\n",
    "    #Assembles vector for input to linear regression model\n",
    "    output = assembler.transform(final)\n",
    "    scaler = MinMaxScaler(inputCol='features',outputCol='sFeatures')\n",
    "    scalerModel = scaler.fit(output)\n",
    "    output = scalerModel.transform(output)\n",
    "    #takes just the scaled Features column and labels to make a two column df for input to linear r\n",
    "    final_data = output.select(['sFeatures', 'Label']).withColumnRenamed('sFeatures','features')\n",
    "    train_data,test_data = final_data.randomSplit([0.7,0.3])\n",
    "    # Create a Linear Regression Model object\n",
    "    lr = LinearRegression(labelCol='Label')\n",
    "    lrModel = lr.fit(train_data)\n",
    "    summary = lrModel.summary \n",
    "    #creates a list of the regression metrics for the client\n",
    "    r2 =summary.r2\n",
    "    RMSE =lrModel.summary.rootMeanSquaredError\n",
    "    datalist = [(c,r2,RMSE,lrModel.summary.pValues[0]\n",
    "                 ,lrModel.summary.pValues[1]\n",
    "                 ,lrModel.summary.pValues[2]\n",
    "                 ,lrModel.summary.pValues[3])]\n",
    "    #converts the regression output list to an RDD, then to a df and unions the df to the main results df\n",
    "    pdatalist = sc.parallelize(datalist)\n",
    "    raw_df = spark.createDataFrame(datalist,schema2)\n",
    "    resultdf = resultdf.union(raw_df)\n",
    "\n",
    "#filters out the literals used to create the inital df and sorts by R Squared\n",
    "resultdf = resultdf.filter('CustomerID<>1').orderBy('r2',ascending=False)\n",
    "#Formats to 4 decimal places\n",
    "for c in resultdf.columns[1:]:\n",
    "             resultdf = resultdf.withColumn(c,bround(c,4))\n",
    "resultdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+---------------+----------+---------------+---------------+\n",
      "|CustomerID|    r2|     RMSE|pVal_L1-S1_Tot3|pVal_Month|pVal_L1-S1_Tot5|pVal_L1-S1_Tot6|\n",
      "+----------+------+---------+---------------+----------+---------------+---------------+\n",
      "|     17841|0.5075| 144.6799|         0.1024|    1.0E-4|         0.3395|         0.3949|\n",
      "|     14527|0.4239|  56.4599|         0.4134|    0.0183|         0.6819|          0.442|\n",
      "|     15039|0.2872| 195.5123|         0.5953|    0.0229|         0.2834|         0.7516|\n",
      "|     13798|0.2496| 605.7141|         0.0229|    0.4618|         0.4169|         0.5994|\n",
      "|     14646|0.1997|5504.3335|         0.8977|    0.7417|         0.0115|         0.8673|\n",
      "|     14156|0.1454|3813.3564|         0.7905|    0.2849|         0.3052|           0.23|\n",
      "|     16422|0.1442| 517.9576|         0.6189|    0.5622|         0.3058|         0.0637|\n",
      "|     14606|0.1313|  49.8456|         0.0212|    0.0216|         0.3328|         0.7299|\n",
      "|     12748|0.1232| 354.2338|         0.1347|    0.0414|         0.6844|         0.1378|\n",
      "|     13408| 0.106|  307.879|         0.6063|    0.4531|         0.1583|         0.1848|\n",
      "|     15311|0.0861| 715.7667|         0.1282|    0.2362|         0.7628|         0.4819|\n",
      "|     12971|0.0792|   93.183|           0.71|    0.6337|         0.2701|         0.8871|\n",
      "|     14911|0.0702| 765.5411|         0.4254|    0.1244|         0.8515|         0.5476|\n",
      "|     13089|0.0541| 685.0214|         0.2533|    0.2686|         0.5976|         0.9046|\n",
      "+----------+------+---------+---------------+----------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultdf = resultdf.filter('CustomerID<>1').orderBy('r2',ascending=False)\n",
    "for c in resultdf.columns[1:]:\n",
    "             resultdf = resultdf.withColumn(c,bround(c,4))\n",
    "resultdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|CustomerID| r2|              RMSE|     pVal_L1-S1_Tot3|          pVal_Month|     pVal_L1-S1_Tot5|    pVal_L1-S1_Tot6|\n",
      "+----------+---+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|         1|2.0|              2.01|                2.01|                2.01|                2.01|               2.01|\n",
      "|     14606|0.0| 51.21707534790039|0.004383583553135395| 0.24977006018161774|  0.6457734107971191| 0.6916141510009766|\n",
      "|     15311|0.0|    514.8115234375|0.021972138434648514|0.008193759247660637| 0.15857551991939545|  0.935386598110199|\n",
      "|     14156|0.0| 2388.231689453125|  0.7416616082191467|  0.8343241810798645|   0.752126157283783|0.09927629679441452|\n",
      "|     14646|0.0|  6070.65380859375| 0.05948681756854057| 0.08887233585119247|0.004989327397197485| 0.7663782238960266|\n",
      "|     15039|0.0|183.70236206054688|  0.7116577625274658|  0.3049057722091675| 0.12001781165599823|0.47110018134117126|\n",
      "|     13408|0.0|209.56175231933594|  0.4246380627155304| 0.25335749983787537|  0.9043102860450745|0.11864899843931198|\n",
      "|     14527|0.0|54.292205810546875| 0.32237622141838074|0.002422467339783907|  0.8778066039085388|0.30838435888290405|\n",
      "|     12748|0.0| 404.9320068359375|   0.811680257320404| 0.10599558055400848| 0.48601004481315613| 0.9230584502220154|\n",
      "|     16422|0.0| 477.2958068847656| 0.14362044632434845| 0.23524077236652374|  0.6762668490409851| 0.7232884764671326|\n",
      "|     13089|0.0| 564.7103271484375|  0.8485780954360962|  0.6545125246047974|  0.2526022493839264|0.49538156390190125|\n",
      "|     14911|0.0|  857.481689453125|  0.1762479692697525| 0.37027785181999207|  0.3549947440624237| 0.2376842200756073|\n",
      "|     17841|0.0|165.88076782226562| 0.08521249890327454|0.012005622498691082| 0.10170970112085342|0.06551510095596313|\n",
      "|     13798|0.0| 572.5926513671875|0.026816051453351974| 0.43061375617980957| 0.06365345418453217|0.28725022077560425|\n",
      "|     12971|0.0|117.92179107666016| 0.24456404149532318|  0.3759517967700958|  0.8780142068862915| 0.9996658563613892|\n",
      "+----------+---+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in dftemplate = dftemplate.withColumn('r2',bround('r2',3))\n",
    "dftemplate.orderBy('r2',ascending=False).show()\n",
    "# Print the coefficients and intercept for linear regression\n",
    "#print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared:        0.199\n",
      "RMSE:             51\n",
      "pValue L1-S1Tot:  0.0019\n",
      "pValue Month:     0.0137\n",
      "pValue L1-S1Avg:  0.9158\n",
      "pValue L2-S2Avg:  0.3117\n",
      "+--------------------+------+------------------+------------------+\n",
      "|            features| Label|        prediction|         Residuals|\n",
      "+--------------------+------+------------------+------------------+\n",
      "|[0.0,1.0,0.450558...|316.79|174.87918741927427|141.91081258072575|\n",
      "|[0.0,1.0,0.450558...|109.88|173.41625074663162|-63.53625074663162|\n",
      "|[0.0,1.0,0.450558...|192.68|172.56201849372488| 20.11798150627513|\n",
      "|[0.0,1.0,0.450558...|215.88|172.56201849372488| 43.31798150627512|\n",
      "|[0.0,1.0,0.450558...|108.63| 158.4970247727116| -49.8670247727116|\n",
      "|[0.09275777647036...|339.68| 193.7762902693422| 145.9037097306578|\n",
      "|[0.26997732060503...|116.06|202.13398147581205|-86.07398147581205|\n",
      "|[0.29574785067658...| 166.3| 198.1464756127939|-31.84647561279388|\n",
      "|[0.29574785067658...|265.18|194.64351470472775| 70.53648529527226|\n",
      "|[0.29574785067658...|164.87|189.66555208838167|-24.79555208838167|\n",
      "+--------------------+------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = lrModel.summary\n",
    "print(\"R squared:        {:.3f}\".format(summary.r2))\n",
    "print(\"RMSE:             {:,.0f}\".format(lrModel.summary.rootMeanSquaredError))\n",
    "print(\"pValue L1-S1Tot:  {:.4f}\".format(lrModel.summary.pValues[0]))\n",
    "print(\"pValue Month:     {:.4f}\".format(lrModel.summary.pValues[1]))\n",
    "print(\"pValue L1-S1Avg:  {:.4f}\".format(lrModel.summary.pValues[2]))\n",
    "print(\"pValue L2-S2Avg:  {:.4f}\".format(lrModel.summary.pValues[3]))\n",
    "rdf = summary.predictions\n",
    "rdf = rdf.withColumn('Residuals', rdf.Label - rdf.prediction)\n",
    "rdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_43fcae7f4e9e7953d8a7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.ml.regression.LinearRegressionTrainingSummary object at 0x7fb840105eb8>\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
