{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes Ecommerce data from Kaggle:\n",
    "    1. calculates input metrics for linear regression: \n",
    "            Total value of transactions over a longer time - total over a shorter time\n",
    "            Month (just uses 1 to 12\n",
    "            Avg spend over longer time vs shorter time\n",
    "            uses two sets of long time/short time values\n",
    "    2. Takes the custoemrs with trading activity on >40 days over the 18 months covered\n",
    "    3. Performs Linear Regression on each to predict trading balance on each day\n",
    "    4. Collects regression metrics into an output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+---------------+----------+---------------+---------------+\n",
      "|CustomerID|  r2|RMSE|pVal_L1-S1_Tot3|pVal_Month|pVal_L1-S1_Tot5|pVal_L1-S1_Tot6|\n",
      "+----------+----+----+---------------+----------+---------------+---------------+\n",
      "|         1|2.01|2.01|           2.01|      2.01|           2.01|           2.01|\n",
      "+----------+----+----+---------------+----------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, IntegerType, StructType, StringType, FloatType\n",
    "import numpy as np\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "df = spark.read.csv('EcommerceData.csv',inferSchema=True,header=True)\n",
    "#dftemplate = spark.read.csv('aargh.csv',inferSchema=True,header=True)\n",
    "#dftemplate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Year and Day of Year column in order to see transactin volume over 10days\n",
    "from pyspark.sql.functions import (format_number,dayofmonth,hour,dayofyear,month,\n",
    "                                   year,weekofyear,date_format,concat, lit, from_unixtime\n",
    "                                   , unix_timestamp, to_date, sum as sm, format_number as fn\n",
    "                                  ,bround, avg as av)\n",
    "from pyspark.sql.types import IntegerType\n",
    "import sys\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Import VectorAssembler and Vectors for use later on\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "df = df.na.drop(subset='CustomerID')\n",
    "df = df.withColumn('Date', to_date(from_unixtime(unix_timestamp(\n",
    "    concat(df['InvoiceDate'],lit('')),format='MM/d/yyyy HH:mm'))))\n",
    "df = df.withColumn('Unix', unix_timestamp(df['Date'],format='yyyy/MM/dd'))\n",
    "df = df.withColumn('Val', df['Quantity']* df['Unitprice'])\n",
    "\n",
    "#remove refunds\n",
    "fdf = df.filter(\"Val is not null and Val >= 0 and StockCode <> 'M'\")\n",
    "\n",
    "fdf = fdf.groupBy(['CustomerID','Date', 'Unix']).agg({'Val':'sum'})\n",
    "fdf = fdf.withColumnRenamed('sum(Val)','Val')\n",
    "fdf = fdf.withColumn('Val', bround(fdf['Val'],scale=2))\n",
    "\n",
    "fdf = fdf.withColumn('DayOfYear', dayofyear(fdf['Date']))\n",
    "fdf = fdf.withColumn('Month', month(fdf['Date']))\n",
    "\n",
    "#define offset window to capture activity in past 10 days\n",
    "days = lambda x: x *86400\n",
    "w = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(28),-days(1))\n",
    "w2 = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(126),-days(1))\n",
    "\n",
    "#Calc average daily spend over short 1 and long 1 periods\n",
    "fdf = fdf.withColumn('S1Tot',bround(sm(fdf['Val']).over(w),scale=2))\n",
    "fdf = fdf.withColumn('L1Tot',bround(sm(fdf['Val']).over(w2),scale=2))\n",
    "fdf = fdf.na.fill(0,subset='S1Tot')\n",
    "fdf = fdf.na.fill(0,subset='L1Tot')\n",
    "fdf = fdf.withColumn('L1-S1Tot', bround(fdf['L1Tot']-fdf['S1Tot'],2))\n",
    "\n",
    "#Calc average daily spend over short 1 and long 1 periods\n",
    "fdf = fdf.withColumn('S1Avg',bround(av(fdf['Val']).over(w),scale=2))\n",
    "fdf = fdf.withColumn('L1Avg',bround(av(fdf['Val']).over(w2),scale=2))\n",
    "fdf = fdf.na.fill(0,subset='S1Avg')\n",
    "fdf = fdf.na.fill(0,subset='L1Avg')\n",
    "fdf = fdf.withColumn('L1-S1Avg', bround(fdf['L1Avg']-fdf['S1Avg'],2))\n",
    "\n",
    "w3 = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(7),-days(1))\n",
    "w4 = Window.partitionBy(fdf['CustomerID']).orderBy(fdf['Unix']).rangeBetween(-days(28),-days(1))\n",
    "\n",
    "#Calc average daily spend over short 1 and long 1 periods\n",
    "fdf = fdf.withColumn('S2Avg',bround(av(fdf['Val']).over(w3),scale=2))\n",
    "fdf = fdf.withColumn('L2Avg',bround(av(fdf['Val']).over(w4),scale=2))\n",
    "fdf = fdf.na.fill(0,subset='S2Avg')\n",
    "fdf = fdf.na.fill(0,subset='L2Avg')\n",
    "fdf = fdf.withColumn('L2-S2Avg', bround(fdf['L2Avg']-fdf['S2Avg'],2))\n",
    "\n",
    "#Create df containing only customers with >40 transactions\n",
    "gred = fdf.groupBy('CustomerID', 'Date').count()\n",
    "gg = gred.groupBy('CustomerID').count().filter('count>39').drop('count')\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    \n",
    "    inputCols=[\"L1-S1Tot\", \"Month\",\"L1-S1Avg\",\"L2-S2Avg\"],\n",
    "    outputCol=\"features\")\n",
    "#gg.count()B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-------------+-----+-------------+-------------+\n",
      "|CustomerID|  r2|RMSE|pVal_L1-S1Tot|Month|pVal_L1-S1Avg|pVal_L2-S2Avg|\n",
      "+----------+----+----+-------------+-----+-------------+-------------+\n",
      "|         1|2.01|2.01|         2.01| 2.01|         2.01|         2.01|\n",
      "+----------+----+----+-------------+-----+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#create list to add columns for results to the 'gg' DF\n",
    "metricCols =['r2', 'RMSE', 'pVal_L1-S1Tot','Month','pVal_L1-S1Avg', 'pVal_L2-S2Avg']\n",
    "\n",
    "columnlist = ['CustomerID'] + metricCols\n",
    "datal = [1,2.01,2.01,2.01,2.01,2.01,2.01]\n",
    "\n",
    "schema1 = [StructField(columnlist[0],IntegerType(),True)\n",
    "           , StructField(columnlist[1], FloatType(),True)\n",
    "          , StructField(columnlist[2], FloatType(),True)\n",
    "          , StructField(columnlist[3], FloatType(),True)\n",
    "          , StructField(columnlist[4], FloatType(),True)\n",
    "          , StructField(columnlist[5], FloatType(),True)\n",
    "          , StructField(columnlist[6], FloatType(),True)\n",
    "            ]\n",
    "\n",
    "dataa = sc.parallelize([datal])\n",
    "schema2 = StructType(fields=schema1)\n",
    "resultdf = spark.createDataFrame(dataa,schema2)\n",
    "resultdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of customers from df containing cust with >40 days of transactions\n",
    "custList = [x[0] for x in gg.select('CustomerID').collect()]\n",
    "#iterate through list running Linear Regression Model for each one\n",
    "for c in custList:\n",
    "    final = fdf.filter(fdf['CustomerID']==c).withColumnRenamed('Val','Label')\n",
    "    final = final.select('L1-S1Tot','Month','L1-S1Avg','L2-S2Avg','Label')\n",
    "    #Assembles vector for input to linear regression model\n",
    "    output = assembler.transform(final)\n",
    "    scaler = MinMaxScaler(inputCol='features',outputCol='sFeatures')\n",
    "    scalerModel = scaler.fit(output)\n",
    "    output = scalerModel.transform(output)\n",
    "    #takes just the scaled Features column and labels to make a two column df for input to linear r\n",
    "    final_data = output.select(['sFeatures', 'Label']).withColumnRenamed('sFeatures','features')\n",
    "    train_data,test_data = final_data.randomSplit([0.7,0.3])\n",
    "    # Create a Linear Regression Model object\n",
    "    lr = LinearRegression(labelCol='Label')\n",
    "    lrModel = lr.fit(train_data)\n",
    "    summary = lrModel.summary \n",
    "    #creates a list of the regression metrics for the client\n",
    "    r2 =summary.r2\n",
    "    RMSE =lrModel.summary.rootMeanSquaredError\n",
    "    datalist = [(c,r2,RMSE,lrModel.summary.pValues[0]\n",
    "                 ,lrModel.summary.pValues[1]\n",
    "                 ,lrModel.summary.pValues[2]\n",
    "                 ,lrModel.summary.pValues[3])]\n",
    "    #converts the regression output list to an RDD, then to a df and unions the df to the main results df\n",
    "    pdatalist = sc.parallelize(datalist)\n",
    "    raw_df = spark.createDataFrame(datalist,schema2)\n",
    "    resultdf = resultdf.union(raw_df)\n",
    "\n",
    "#filters out the literals used to create the inital df and sorts by R Squared\n",
    "resultdf = resultdf.filter('CustomerID<>1').orderBy('r2',ascending=False)\n",
    "#Formats to 4 decimal places\n",
    "for c in resultdf.columns[1:]:\n",
    "             resultdf = resultdf.withColumn(c,bround(c,4))\n",
    "resultdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+---------------+----------+---------------+---------------+\n",
      "|CustomerID|    r2|     RMSE|pVal_L1-S1_Tot3|pVal_Month|pVal_L1-S1_Tot5|pVal_L1-S1_Tot6|\n",
      "+----------+------+---------+---------------+----------+---------------+---------------+\n",
      "|     17841|0.5075| 144.6799|         0.1024|    1.0E-4|         0.3395|         0.3949|\n",
      "|     14527|0.4239|  56.4599|         0.4134|    0.0183|         0.6819|          0.442|\n",
      "|     15039|0.2872| 195.5123|         0.5953|    0.0229|         0.2834|         0.7516|\n",
      "|     13798|0.2496| 605.7141|         0.0229|    0.4618|         0.4169|         0.5994|\n",
      "|     14646|0.1997|5504.3335|         0.8977|    0.7417|         0.0115|         0.8673|\n",
      "|     14156|0.1454|3813.3564|         0.7905|    0.2849|         0.3052|           0.23|\n",
      "|     16422|0.1442| 517.9576|         0.6189|    0.5622|         0.3058|         0.0637|\n",
      "|     14606|0.1313|  49.8456|         0.0212|    0.0216|         0.3328|         0.7299|\n",
      "|     12748|0.1232| 354.2338|         0.1347|    0.0414|         0.6844|         0.1378|\n",
      "|     13408| 0.106|  307.879|         0.6063|    0.4531|         0.1583|         0.1848|\n",
      "|     15311|0.0861| 715.7667|         0.1282|    0.2362|         0.7628|         0.4819|\n",
      "|     12971|0.0792|   93.183|           0.71|    0.6337|         0.2701|         0.8871|\n",
      "|     14911|0.0702| 765.5411|         0.4254|    0.1244|         0.8515|         0.5476|\n",
      "|     13089|0.0541| 685.0214|         0.2533|    0.2686|         0.5976|         0.9046|\n",
      "+----------+------+---------+---------------+----------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultdf = resultdf.filter('CustomerID<>1').orderBy('r2',ascending=False)\n",
    "for c in resultdf.columns[1:]:\n",
    "             resultdf = resultdf.withColumn(c,bround(c,4))\n",
    "resultdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|CustomerID| r2|              RMSE|     pVal_L1-S1_Tot3|          pVal_Month|     pVal_L1-S1_Tot5|    pVal_L1-S1_Tot6|\n",
      "+----------+---+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|         1|2.0|              2.01|                2.01|                2.01|                2.01|               2.01|\n",
      "|     14606|0.0| 51.21707534790039|0.004383583553135395| 0.24977006018161774|  0.6457734107971191| 0.6916141510009766|\n",
      "|     15311|0.0|    514.8115234375|0.021972138434648514|0.008193759247660637| 0.15857551991939545|  0.935386598110199|\n",
      "|     14156|0.0| 2388.231689453125|  0.7416616082191467|  0.8343241810798645|   0.752126157283783|0.09927629679441452|\n",
      "|     14646|0.0|  6070.65380859375| 0.05948681756854057| 0.08887233585119247|0.004989327397197485| 0.7663782238960266|\n",
      "|     15039|0.0|183.70236206054688|  0.7116577625274658|  0.3049057722091675| 0.12001781165599823|0.47110018134117126|\n",
      "|     13408|0.0|209.56175231933594|  0.4246380627155304| 0.25335749983787537|  0.9043102860450745|0.11864899843931198|\n",
      "|     14527|0.0|54.292205810546875| 0.32237622141838074|0.002422467339783907|  0.8778066039085388|0.30838435888290405|\n",
      "|     12748|0.0| 404.9320068359375|   0.811680257320404| 0.10599558055400848| 0.48601004481315613| 0.9230584502220154|\n",
      "|     16422|0.0| 477.2958068847656| 0.14362044632434845| 0.23524077236652374|  0.6762668490409851| 0.7232884764671326|\n",
      "|     13089|0.0| 564.7103271484375|  0.8485780954360962|  0.6545125246047974|  0.2526022493839264|0.49538156390190125|\n",
      "|     14911|0.0|  857.481689453125|  0.1762479692697525| 0.37027785181999207|  0.3549947440624237| 0.2376842200756073|\n",
      "|     17841|0.0|165.88076782226562| 0.08521249890327454|0.012005622498691082| 0.10170970112085342|0.06551510095596313|\n",
      "|     13798|0.0| 572.5926513671875|0.026816051453351974| 0.43061375617980957| 0.06365345418453217|0.28725022077560425|\n",
      "|     12971|0.0|117.92179107666016| 0.24456404149532318|  0.3759517967700958|  0.8780142068862915| 0.9996658563613892|\n",
      "+----------+---+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in dftemplate = dftemplate.withColumn('r2',bround('r2',3))\n",
    "dftemplate.orderBy('r2',ascending=False).show()\n",
    "# Print the coefficients and intercept for linear regression\n",
    "#print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared:        0.199\n",
      "RMSE:             51\n",
      "pValue L1-S1Tot:  0.0019\n",
      "pValue Month:     0.0137\n",
      "pValue L1-S1Avg:  0.9158\n",
      "pValue L2-S2Avg:  0.3117\n",
      "+--------------------+------+------------------+------------------+\n",
      "|            features| Label|        prediction|         Residuals|\n",
      "+--------------------+------+------------------+------------------+\n",
      "|[0.0,1.0,0.450558...|316.79|174.87918741927427|141.91081258072575|\n",
      "|[0.0,1.0,0.450558...|109.88|173.41625074663162|-63.53625074663162|\n",
      "|[0.0,1.0,0.450558...|192.68|172.56201849372488| 20.11798150627513|\n",
      "|[0.0,1.0,0.450558...|215.88|172.56201849372488| 43.31798150627512|\n",
      "|[0.0,1.0,0.450558...|108.63| 158.4970247727116| -49.8670247727116|\n",
      "|[0.09275777647036...|339.68| 193.7762902693422| 145.9037097306578|\n",
      "|[0.26997732060503...|116.06|202.13398147581205|-86.07398147581205|\n",
      "|[0.29574785067658...| 166.3| 198.1464756127939|-31.84647561279388|\n",
      "|[0.29574785067658...|265.18|194.64351470472775| 70.53648529527226|\n",
      "|[0.29574785067658...|164.87|189.66555208838167|-24.79555208838167|\n",
      "+--------------------+------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = lrModel.summary\n",
    "print(\"R squared:        {:.3f}\".format(summary.r2))\n",
    "print(\"RMSE:             {:,.0f}\".format(lrModel.summary.rootMeanSquaredError))\n",
    "print(\"pValue L1-S1Tot:  {:.4f}\".format(lrModel.summary.pValues[0]))\n",
    "print(\"pValue Month:     {:.4f}\".format(lrModel.summary.pValues[1]))\n",
    "print(\"pValue L1-S1Avg:  {:.4f}\".format(lrModel.summary.pValues[2]))\n",
    "print(\"pValue L2-S2Avg:  {:.4f}\".format(lrModel.summary.pValues[3]))\n",
    "rdf = summary.predictions\n",
    "rdf = rdf.withColumn('Residuals', rdf.Label - rdf.prediction)\n",
    "rdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_43fcae7f4e9e7953d8a7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.ml.regression.LinearRegressionTrainingSummary object at 0x7fb840105eb8>\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
